[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Add title here",
    "section": "",
    "text": "Add website description here\n\nDisclaimer\nThe content on this website is my sole responsibility and does not reflect the views of any other individual or organization. I make no claims about the accuracy or completeness of the information provided. Use the website at your own risk. I am not liable for any damages arising from your use of this website.\nSome content on this website has been generated using artificial intelligence (AI) tools. While the AI assists in creating content, the final output is always subject to supervision and refinement by the author. I strive to ensure accuracy and quality in all my content through careful oversight and human involvement."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "whitepaper.html",
    "href": "whitepaper.html",
    "title": "White Paper",
    "section": "",
    "text": "Summary: The Experience of Serious Risk-Taking Behaviors in Bipolar Disorders: Exploring the Potential for Co-Design Resources within Community Support Groups.\nGuillermo Perez Algorta, David Dinham & Emmanuel Tsekleves\nBipolar Disorder (BD) impacts millions globally, causing intense mood swings and risky behaviors. People with BD often engage in actions like substance misuse and financial recklessness, leading to prolonged mood issues and even life-threatening situations. However, existing interventions mainly focus on mood regulation, leaving a gap in directly addressing these risky behaviors.\nOur proposal offers a fresh perspective by introducing co-design to create resources for managing these behaviors. Inspired by Fuzzy Trace Theory (FTT), we shift the focus from detailed reasoning to a simpler, more qualitative approach. The key idea is to encourage individuals to grasp the essential meaning of daily situations involving risk and connect it with their personal values.\nIn practical terms, this means training individuals to use “gist” reasoning, to navigate situations involving risk. Our approach is guided by four themes drawn from real-life experiences of individuals with BD: “True Self and Risk,” “Empowerment, Sense of Control and Self-Compassion,” “Intolerance of Uncertainty in the Context of Risk,” and “Systemic-Relational Aspects Relevant to Risk.”\nCommunity Support Groups (CSG) could play a crucial role in implementing these activities. These groups, rooted in asset-based approaches, empower participants to thrive despite BD challenges. They offer inclusive opportunities for diverse learning, communication, and shared experiences.\nThe co-design approach is essential for developing interventions. By incorporating lived expertise from individuals with BD alongside clinical and research perspectives, we aim to create solutions that resonate with those they intend to benefit.\nOur “phygital” space, inspired by community allotments and gardens, will combine physical and online environments, fostering collaboration, inquiry, and knowledge exchange. This approach will allow us to address complex health issues related to BD effectively.\nIn essence, our proposal seeks to transform the narrative around risky behaviors in BD. By emphasizing simple understanding of risk, integrating personal values, and leveraging community support groups, we aim to empower individuals to actively manage their behaviors. The co-design methodology ensures interventions are co-created with the very individuals they aim to benefit, reflecting a commitment to holistic and impactful solutions for those navigating the complexities of BD.\n\nClick here to access White Paper full version."
  },
  {
    "objectID": "index.html#who-we-are",
    "href": "index.html#who-we-are",
    "title": "The Experience of Serious Risk-Taking Behaviors in Bipolar Disorders",
    "section": "Who we are?",
    "text": "Who we are?\nGuillermo Perez Algorta has been leading on a line of research on the topic of risk behavior management in daily life in people with a diagnosis of bipolar disorder.\nDD, Bipolar Edinburgh,\nET, an expert leading on community-based health research projects through co-design methodologies.\n\n\nThis project has been supported by Lancaster University Research Catalyst Fund (2022/2023)"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Add list of resources here"
  },
  {
    "objectID": "posts/my_first_post/index.html",
    "href": "posts/my_first_post/index.html",
    "title": "Sobre las ideas negativas",
    "section": "",
    "text": "Están ahí. Cerca. Te das vuelta y te tiran el tarascón.\nNo hay mucho para hacer. Esperar que se alejen, que se vallan por donde vinieron.\nPredadores con hambre. Pegás un grito, un gesto con la mano, y sí, se alejan por un rato. Se hechan cerca y te miran. Al rato, como si nada, arrancan de nuevo tratando de garronear, de meter una mordida.\nSi hay fuego, tenes que mantenerlo y no alejarte mucho.\nHacer ruido ayuda. He visto gente hacer colgantes con botellas de vidrio. Obvio que necesitas viento.\nSi hay más de uno, es bueno para hacer turnos. Parece que lo que buscan es desgastarte. Te cansan, te juegan a largo plazo.\n {fig-align=“right”width=20%}"
  },
  {
    "objectID": "posts/my_first_post/index.html#this-is-a-second-level-heading",
    "href": "posts/my_first_post/index.html#this-is-a-second-level-heading",
    "title": "My First post",
    "section": "This is a second level heading",
    "text": "This is a second level heading\nYou can include images, and many other kinds of content.\nIf you are using the visual editor in Rstudio you can drag images onto the editor to insert them into the document. Otherwise, you need to place the image inside of the folder for this post, and then you can insert it to your post directly, like this:\n\nThe first image in a blog post will also be used on the listings page."
  },
  {
    "objectID": "posts/Example_assignment/index.html",
    "href": "posts/Example_assignment/index.html",
    "title": "Blog title",
    "section": "",
    "text": "Add content here."
  },
  {
    "objectID": "posts/Example_assignment/index.html#my-mental-imagery",
    "href": "posts/Example_assignment/index.html#my-mental-imagery",
    "title": "Example assignment",
    "section": "My mental imagery",
    "text": "My mental imagery\nMy mental imagery is like…"
  },
  {
    "objectID": "posts/second_post/index.html",
    "href": "posts/second_post/index.html",
    "title": "second post",
    "section": "",
    "text": "A test post where I change something."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "GPA Blog",
    "section": "",
    "text": "Sobre las ideas negativas\n\n\n\n\n\n\n\ngeneral\n\n\nspanish\n\n\nmental health\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nGPA\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "AI and Mental Health. (under development)",
    "section": "",
    "text": "In comparison to related fields like education, the field of Mental Health clinical practice has witnessed limited technological transformation. From clinicians of the late 19th century to contemporary practitioners, clinical activities have centered around language. However, for the first time, this situation might be on the verge of change.\nThe rapid development of artificial intelligence tools with the capacity to emulate human language holds the potential to revolutionize Mental Health clinical practice. At present, these tools may come across as deceptive, non-empathic oracles without agency. Nevertheless, their design, such as typing responses character by character, readily encourages the attribution of awareness, fostering the impression of genuine human interaction.\nForecasting the evolution of these tools poses a challenge, yet it can be asserted with certainty that these tools are here to stay.\nOur focus is centered on comprehending the intricate relationship between AI tools and the field of Mental Health clinical practice. Through this endeavour, our primary objective is to identify the challenges and opportunities stemming from this interaction and to formulate a series of recommendations aimed at mitigating negative outcomes and maximizing prospects within this field.\nThis challenge involves various stakeholders at different levels. It is important to note that while the following list is not exhaustive, it represents a preliminary exploration encompassing:\n- The impact on training, including continued education and accreditation.\n- The regulation of clinical practice (e.g., implications for practitioners’ code of conduct).\n- The regulation of clinical research practices (e.g., considerations pertaining to Institutional Review Boards).\n- Strategies for disseminating and engaging with relevant stakeholders, including efforts to enhance AI literacy within the context of mental health clinical practice.\n- Considerations regarding the effects on diversity and equality policies related to access to mental health support.\nIn light of these profound changes on the horizon, we call for a comprehensive approach that includes:\n- The development of specialized training programs, workshops, and certifications in AI-assisted Mental Health practice to ensure that clinicians are proficient in utilizing these tools.\n- The establishment of clear ethical guidelines and standards of practice, incorporating AI interactions and communication as integral components of practitioners’ code of conduct.\n- Rigorous oversight and evaluation mechanisms for AI-driven research practices, ensuring that ethical considerations, privacy, and data security are upheld, with the guidance of Institutional Review Boards.\n- Active engagement with stakeholders, including policymakers, healthcare professionals, and AI developers, to facilitate the dissemination of AI literacy in the context of mental health clinical practice.\n- The implementation of policies and initiatives aimed at fostering diversity and equity, thereby promoting accessible mental health support for all, including underrepresented populations around the world.\nAs we stand at the beginning of a new era in Mental Health clinical practice, these concrete steps are essential in shaping a future where AI enhances, rather than diminishes, the quality and accessibility of mental health care.\n\n\nOne way to articulate this relationship involves considering the different levels of contact between AI tools and service users (SUs) (Table 1).\nOn one end of the spectrum, we find entirely human interactions between health agents and service users. As we move along the spectrum, a third agent, the AI tool, becomes involved in this clinical relationship, eventually reaching the opposite end where the health human agent disappears, leaving only the AI tool and the service user.\nThe primary focus of this reflective exercise lies in decision-making. As described below, the transition spans from purely human decision-making to shared decision-making and, finally, to purely AI decision making.\nA second approach to reflect on this relationship involves evaluating how various capacities (cognitive, affective, manual, and moral) are implemented across the spectrum described above.\nTo do this, we adopt both an anthropocentric position and what we term a deceptive position.\nIn the deceptive stance, having agency is not prerequisite for the AI tool. Here, the critical factor is the AI tool's ability to imitate and persuade the service user that it possesses all four capabilities.\nTable here?\nCo-pilot: At this level, all final decisions are human-based decisions. The role of AI tools is restricted to providing support to human clinical practice. In this clinical scenario, the service user (SU) has no direct contact with the AI tool but can be aware of its role in the treatment process. Examples of Co-pilot may include assessment routines, where collection, analysis, and reports based on SU data are provided for clinicians. We could also observe treatment recommendations based on clinicians’ prompts, for example, suggesting treatment routines to tackle issues derived from combinations of clinical vignettes and assessment inputs.\nThe risk for humans here would be low.\nAugmented: Here, the clinician and AI tools work in parallel, sharing decisions. The service user is informed from the outset about the existence of AI tools and the possibility of having direct interactions with AI without total supervision from the clinician. In this scenario, SU consent would be a requirement. The clinician will attempt to have close monitoring of these AI interventions. Clinicians should invest in patient education regarding the risks and benefits of interacting with AI tools. An example of this could be homework prescribed by the clinician, decided and guided by AI tools, that is carried out outside clinical hours. The clinician may request (or have access to) records of these interactions, with the possibility of tailoring prompts to refine future interactions.\nThe level of risk is medium.\nSupervised: In this case, all clinical decisions are AI-based, supervised by humans. The SU interacts with AI without direct clinician involvement. The SU might receive training on risks and benefits, but AI tools are entirely autonomous when interacting with SUs. The clinician serves as an external supervisor (e.g., Gessel camera observation) where they can access samples of interactions between AI and SUs but no longer have the capability to intervene in real time. These assessments conducted by clinicians could be used to adjust global AI parameters (calibration) to improve the quality performance of AI tools with future SUs.\nThe level of risk is high.\nAutonomous: This is the most extreme scenario where the clinician’s role disappears. Training resources for SUs interacting with AI tools can be provided, but it is not a prerequisite for accessing these AI tools. There is not necessarily control over the implementation of practices and results (lack of audits or quality control).\nThe level of risk is very high."
  },
  {
    "objectID": "projects.html#position-paper-primum-non-nocere-opportunities-and-challenges-of-integrating-ai-tools-in-contemporary-mental-health-clinical-practice.",
    "href": "projects.html#position-paper-primum-non-nocere-opportunities-and-challenges-of-integrating-ai-tools-in-contemporary-mental-health-clinical-practice.",
    "title": "AI and Mental Health. (under development)",
    "section": "",
    "text": "In comparison to related fields like education, the field of Mental Health clinical practice has witnessed limited technological transformation. From clinicians of the late 19th century to contemporary practitioners, clinical activities have centered around language. However, for the first time, this situation might be on the verge of change.\nThe rapid development of artificial intelligence tools with the capacity to emulate human language holds the potential to revolutionize Mental Health clinical practice. At present, these tools may come across as deceptive, non-empathic oracles without agency. Nevertheless, their design, such as typing responses character by character, readily encourages the attribution of awareness, fostering the impression of genuine human interaction.\nForecasting the evolution of these tools poses a challenge, yet it can be asserted with certainty that these tools are here to stay.\nOur focus is centered on comprehending the intricate relationship between AI tools and the field of Mental Health clinical practice. Through this endeavour, our primary objective is to identify the challenges and opportunities stemming from this interaction and to formulate a series of recommendations aimed at mitigating negative outcomes and maximizing prospects within this field.\nThis challenge involves various stakeholders at different levels. It is important to note that while the following list is not exhaustive, it represents a preliminary exploration encompassing:\n- The impact on training, including continued education and accreditation.\n- The regulation of clinical practice (e.g., implications for practitioners’ code of conduct).\n- The regulation of clinical research practices (e.g., considerations pertaining to Institutional Review Boards).\n- Strategies for disseminating and engaging with relevant stakeholders, including efforts to enhance AI literacy within the context of mental health clinical practice.\n- Considerations regarding the effects on diversity and equality policies related to access to mental health support.\nIn light of these profound changes on the horizon, we call for a comprehensive approach that includes:\n- The development of specialized training programs, workshops, and certifications in AI-assisted Mental Health practice to ensure that clinicians are proficient in utilizing these tools.\n- The establishment of clear ethical guidelines and standards of practice, incorporating AI interactions and communication as integral components of practitioners’ code of conduct.\n- Rigorous oversight and evaluation mechanisms for AI-driven research practices, ensuring that ethical considerations, privacy, and data security are upheld, with the guidance of Institutional Review Boards.\n- Active engagement with stakeholders, including policymakers, healthcare professionals, and AI developers, to facilitate the dissemination of AI literacy in the context of mental health clinical practice.\n- The implementation of policies and initiatives aimed at fostering diversity and equity, thereby promoting accessible mental health support for all, including underrepresented populations around the world.\nAs we stand at the beginning of a new era in Mental Health clinical practice, these concrete steps are essential in shaping a future where AI enhances, rather than diminishes, the quality and accessibility of mental health care.\n\n\nOne way to articulate this relationship involves considering the different levels of contact between AI tools and service users (SUs) (Table 1).\nOn one end of the spectrum, we find entirely human interactions between health agents and service users. As we move along the spectrum, a third agent, the AI tool, becomes involved in this clinical relationship, eventually reaching the opposite end where the health human agent disappears, leaving only the AI tool and the service user.\nThe primary focus of this reflective exercise lies in decision-making. As described below, the transition spans from purely human decision-making to shared decision-making and, finally, to purely AI decision making.\nA second approach to reflect on this relationship involves evaluating how various capacities (cognitive, affective, manual, and moral) are implemented across the spectrum described above.\nTo do this, we adopt both an anthropocentric position and what we term a deceptive position.\nIn the deceptive stance, having agency is not prerequisite for the AI tool. Here, the critical factor is the AI tool's ability to imitate and persuade the service user that it possesses all four capabilities.\nTable here?\nCo-pilot: At this level, all final decisions are human-based decisions. The role of AI tools is restricted to providing support to human clinical practice. In this clinical scenario, the service user (SU) has no direct contact with the AI tool but can be aware of its role in the treatment process. Examples of Co-pilot may include assessment routines, where collection, analysis, and reports based on SU data are provided for clinicians. We could also observe treatment recommendations based on clinicians’ prompts, for example, suggesting treatment routines to tackle issues derived from combinations of clinical vignettes and assessment inputs.\nThe risk for humans here would be low.\nAugmented: Here, the clinician and AI tools work in parallel, sharing decisions. The service user is informed from the outset about the existence of AI tools and the possibility of having direct interactions with AI without total supervision from the clinician. In this scenario, SU consent would be a requirement. The clinician will attempt to have close monitoring of these AI interventions. Clinicians should invest in patient education regarding the risks and benefits of interacting with AI tools. An example of this could be homework prescribed by the clinician, decided and guided by AI tools, that is carried out outside clinical hours. The clinician may request (or have access to) records of these interactions, with the possibility of tailoring prompts to refine future interactions.\nThe level of risk is medium.\nSupervised: In this case, all clinical decisions are AI-based, supervised by humans. The SU interacts with AI without direct clinician involvement. The SU might receive training on risks and benefits, but AI tools are entirely autonomous when interacting with SUs. The clinician serves as an external supervisor (e.g., Gessel camera observation) where they can access samples of interactions between AI and SUs but no longer have the capability to intervene in real time. These assessments conducted by clinicians could be used to adjust global AI parameters (calibration) to improve the quality performance of AI tools with future SUs.\nThe level of risk is high.\nAutonomous: This is the most extreme scenario where the clinician’s role disappears. Training resources for SUs interacting with AI tools can be provided, but it is not a prerequisite for accessing these AI tools. There is not necessarily control over the implementation of practices and results (lack of audits or quality control).\nThe level of risk is very high."
  }
]